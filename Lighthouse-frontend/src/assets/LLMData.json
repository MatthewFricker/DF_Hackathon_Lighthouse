[
  {
    "type": "model",
    "name": "GPT-4",
    "organization": "OpenAI",
    "description": "GPT-4 is OpenAI's most advanced system, producing safer and more useful responses",
    "created_date": "2023-03-14",
    "url": "https://arxiv.org/abs/2303.08774",
    "datasheet": "",
    "modality": "image, text; image, text",
    "size": "unknown",
    "sample": "",
    "analysis": "",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "limited",
    "license": "unknown",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "",
    "model_card": "",
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Gemini",
    "organization": "Google",
    "description": "As of release, Gemini is Google's most capable and flexible AI model, proficient in multimodal domains.",
    "created_date": "2023-12-06",
    "url": "https://deepmind.google/technologies/gemini/#introduction",
    "datasheet": "",
    "modality": "text; image, text, video",
    "size": "unknown",
    "sample": "",
    "analysis": "Evaluated on standard general, reasoning, math, coding, and multimodal benchmarks with results that surpass GPT-4 on almost all.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "closed",
    "license": "unknown",
    "intended_uses": "general use large language model that can be used for language, reasoning, and code tasks.",
    "prohibited_uses": "becoming part of a general-purpose service or product or use within specific downstream applications without prior assessment",
    "monitoring": "Google internal monitoring",
    "feedback": "",
    "model_card": "",
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "unknown",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Llama 3",
    "organization": "Meta",
    "description": "Llama 3 is the third generation of Meta AI's open-source large language model. It comes with pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases.",
    "created_date": "2024-04-18",
    "url": "https://llama.meta.com/llama3/",
    "datasheet": "",
    "modality": "text; text",
    "size": "70B parameters",
    "sample": "",
    "analysis": "The models were evaluated based on their performance on standard benchmarks and real-world scenarios. These evaluations were performed using a high-quality human evaluation set containing 1,800 prompts covering multiple use cases. The models also went through red-teaming for safety, where human experts and automated methods were used to generate adversarial prompts to test for problematic responses.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "Extensive internal and external testing for safety, and design of new trust and safety tools.",
    "access": "open",
    "license": "Llama 3",
    "intended_uses": "Llama 3 is intended for a broad range of use cases, including AI assistance, content creation, learning, and analysis.",
    "prohibited_uses": "unknown",
    "monitoring": "Extensive internal and external performance evaluation and red-teaming approach for safety testing.",
    "feedback": "Feedback is encouraged from users to improve the model, but the feedback mechanism is not explicitly described.",
    "model_card": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md",
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "2 custom-built Meta 24K GPU clusters",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Vicuna",
    "organization": "LMSYS",
    "description": "An open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.",
    "created_date": "2023-03-30",
    "url": "https://lmsys.org/blog/2023-03-30-vicuna/",
    "datasheet": "",
    "modality": "text; text",
    "size": "13B parameters (dense)",
    "sample": "",
    "analysis": "Evaluated against similar LLMs using GPT-4 as a judge.",
    "dependencies": "['LLaMA', 'ShareGPT conversations data']",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "research on LLMs and chatbots",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/datasets/bigcode/the-stack/discussions",
    "model_card": "https://huggingface.co/lmsys/vicuna-13b-delta-v0",
    "training_emissions": "",
    "training_time": "1 day",
    "training_hardware": "8 A100 GPUs",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Claude 3",
    "organization": "Anthropic",
    "description": "The Claude 3 model family is a collection of models which sets new industry benchmarks across a wide range of cognitive tasks.",
    "created_date": "2024-03-04",
    "url": "https://www.anthropic.com/news/claude-3-family",
    "datasheet": "",
    "modality": "image, text; text",
    "size": "unknown",
    "sample": "",
    "analysis": "Evaluated on reasoning, math, coding, reading comprehension, and question answering, outperforming GPT-4 on standard benchmarks.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "Pre-trained on diverse dataset and aligned with Constitutional AI technique.",
    "access": "limited",
    "license": "unknown",
    "intended_uses": "Claude models excel at open-ended conversation and collaboration on ideas, and also perform exceptionally well in coding tasks and when working with text - whether searching, writing, editing, outlining, or summarizing.",
    "prohibited_uses": "Prohibited uses include, but are not limited to, political campaigning or lobbying, surveillance, social scoring, criminal justice decisions, law enforcement, and decisions related to financing, employment, and housing.",
    "monitoring": "",
    "feedback": "",
    "model_card": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "unknown",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "StableLM 2",
    "organization": "Stability AI",
    "description": "StableLM 2 is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
    "created_date": "2024-01-19",
    "url": "https://stability.ai/news/introducing-stable-lm-2",
    "datasheet": "",
    "modality": "text; text",
    "size": "1.6B parameters (dense)",
    "sample": "",
    "analysis": "Evaluated on standard LLM benchmarks and in multilingual tasks compared to similarly sized open-source models.",
    "dependencies": "['RedPajama-Data', 'The Pile', 'RefinedWeb', 'The Stack', 'OpenWebText', 'OpenWebMath']",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "open",
    "license": "custom",
    "intended_uses": "The model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications.",
    "prohibited_uses": "",
    "monitoring": "unknown",
    "feedback": "https://huggingface.co/stabilityai/stablelm-2-1_6b/discussions",
    "model_card": "https://huggingface.co/stabilityai/stablelm-2-1_6b",
    "training_emissions": "11 tCO2eq",
    "training_time": "92k GPU hours",
    "training_hardware": "512 NVIDIA A100 40GB GPUs",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.",
    "created_date": "2023-09-06",
    "url": "https://arxiv.org/pdf/2311.16867.pdf",
    "datasheet": "",
    "modality": "text; text",
    "size": "180B parameters (dense)",
    "sample": "",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "dependencies": "['RefinedWeb']",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "open",
    "license": "unknown",
    "intended_uses": "Research on large language models; as a foundation for further specialization for specific use cases.",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "None",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "DBRX",
    "organization": "Databricks",
    "description": "DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction by Databricks. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. DBRX only accepts text-based inputs and produces text-based outputs.",
    "created_date": "2024-03-27",
    "url": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
    "datasheet": "",
    "modality": "text; text",
    "size": "132B parameters (sparse)",
    "sample": "",
    "analysis": "DBRX outperforms established open-source and open-weight base models on the Databricks Model Gauntlet, the Hugging Face Open LLM Leaderboard, and HumanEval. Full evaluation details can be found in the corresponding technical blog post.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "Recommendations provided for retrieval augmented generation (RAG) in scenarios where accuracy and fidelity are important and additional testing around safety in the context of the specific application and domain is suggested.",
    "access": "open",
    "license": "Databricks Open Model License",
    "intended_uses": "DBRX models are open, general-purpose LLMs intended and licensed for both commercial and research applications. They can be further fine-tuned for various domain-specific natural language and coding tasks.",
    "prohibited_uses": "DBRX models are not intended to be used out-of-the-box in non-English languages, and do not support native code execution, function calling or any use that violates applicable laws or regulations or is otherwise prohibited by the Databricks Open Model License and Databricks Open Model Acceptable Use Policy.",
    "monitoring": "unknown",
    "feedback": "https://huggingface.co/databricks/dbrx-base/discussions",
    "model_card": "https://huggingface.co/databricks/dbrx-base",
    "training_emissions": "unknown",
    "training_time": "3 months",
    "training_hardware": "3072 NVIDIA H100s connected by 3.2Tbps Infiniband",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Mistral Large",
    "organization": "Mistral AI",
    "description": "Mistral Large is Mistral AI's new cutting-edge text generation model.",
    "created_date": "2024-02-26",
    "url": "https://mistral.ai/news/mistral-large/",
    "datasheet": "",
    "modality": "text; text",
    "size": "unknown",
    "sample": "",
    "analysis": "Evaluated on commonly used benchmarks in comparison to the current LLM leaders.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "limited",
    "license": "unknown",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "",
    "model_card": "",
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "unknown",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  },
  {
    "type": "model",
    "name": "Grok-1.5V",
    "organization": "xAI",
    "description": "Grok-1.5V is a first-generation multimodal model which can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.",
    "created_date": "2024-04-12",
    "url": "https://x.ai/blog/grok-1.5v",
    "datasheet": "",
    "modality": "image, text; text",
    "size": "unknown",
    "sample": "",
    "analysis": "The model is evaluated in a zero-shot setting without chain-of-thought prompting. The evaluation domains include multi-disciplinary reasoning, understanding documents, science diagrams, charts, screenshots, photographs and real-world spatial understanding. The model shows competitive performance with existing frontier multimodal models.",
    "dependencies": "[]",
    "included": "",
    "excluded": "",
    "quality_control": "",
    "access": "limited",
    "license": "unknown",
    "intended_uses": "Grok-1.5V can be used for understanding documents, science diagrams, charts, screenshots, photographs. It can also translate diagrams into Python code.",
    "prohibited_uses": "unknown",
    "monitoring": "unknown",
    "feedback": "",
    "model_card": "",
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "unknown",
    "adaptation": "",
    "output_space": "",
    "terms_of_service": "",
    "monthly_active_users": "",
    "user_distribution": "",
    "failures": ""
  }
] 
